{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "Pipeline_Id = 1            \r\n",
        "Pipeline_Name = \"LoadingDataFromSourceToSilver\" \r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool",
              "statement_id": 2,
              "statement_ids": [
                2
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T12:40:31.4431279Z",
              "session_start_time": "2024-12-01T12:40:31.4811216Z",
              "execution_start_time": "2024-12-01T12:41:22.9816439Z",
              "execution_finish_time": "2024-12-01T12:41:23.1963562Z",
              "parent_msg_id": "ba96ea61-4ad8-48c7-8b61-88489c04d426"
            },
            "text/plain": "StatementMeta(sparkpool, 1, 2, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jdbc_url = \"jdbc:sqlserver://sqlserver8341.database.windows.net:1433;database=GoldLayer\"\r\n",
        "connection_properties = {\r\n",
        "    \"user\": \"suneetha\",\r\n",
        "    \"password\": \"Suni@123\",\r\n",
        "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
        "}"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool",
              "statement_id": 3,
              "statement_ids": [
                3
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T11:09:35.097627Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T11:09:35.2319172Z",
              "execution_finish_time": "2024-12-01T11:09:35.4052676Z",
              "parent_msg_id": "4151e2e5-1637-4faa-9c69-7ced87261006"
            },
            "text/plain": "StatementMeta(sparkpool, 0, 3, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.functions import regexp_replace,col,substring, substring_index, instr,concat_ws,md5,to_timestamp,lit,monotonically_increasing_id, lit,current_timestamp\r\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType,DateType,TimestampType\r\n",
        "import re\r\n",
        "\r\n",
        "source_schema = StructType([\r\n",
        "                        StructField(\"id\", IntegerType(), True),\r\n",
        "                        StructField(\"batter\", StringType(), True),\r\n",
        "                        StructField(\"bowler\", StringType(), True),\r\n",
        "                        StructField(\"non_striker\", StringType(), True),\r\n",
        "                        StructField(\"runs_batter\", IntegerType(), True),\r\n",
        "                        StructField(\"runs_extras\", IntegerType(), True),\r\n",
        "                        StructField(\"runs_total\", IntegerType(), True),\r\n",
        "                        StructField(\"delivery_number\", IntegerType(), True),\r\n",
        "                        StructField(\"over_number\", IntegerType(), True),  \r\n",
        "                        StructField(\"team\", StringType(), True),\r\n",
        "                        StructField(\"inning_number\", IntegerType(), True),\r\n",
        "                        StructField(\"match_name\", StringType(), True),\r\n",
        "                        StructField(\"match_id\", StringType(), True),\r\n",
        "                        StructField(\"match_city\", StringType(), True),\r\n",
        "                        StructField(\"match_venue\", StringType(), True),\r\n",
        "                        StructField(\"match_type\", StringType(), True),\r\n",
        "                        StructField(\"team_type\", StringType(), True),\r\n",
        "                        StructField(\"match_start_date\", DateType(), True),\r\n",
        "                        StructField(\"player_out\", StringType(), True),\r\n",
        "                        StructField(\"_corrupt_record\", StringType(), True)\r\n",
        "\r\n",
        "                    ])\r\n"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_files():\r\n",
        "        mssparkutils.fs.mount(\r\n",
        "            \"abfss://bronzelayer@gen2storageforazurepoc.dfs.core.windows.net/\",\r\n",
        "            \"/mountname\",\r\n",
        "            {\"linkedService\" : \"synapsepoc123-WorkspaceDefaultStorage\"}\r\n",
        "        )\r\n",
        "\r\n",
        "        files = mssparkutils.fs.ls(f'file:{mssparkutils.fs.getMountPath(\"/mountname\")}')\r\n",
        "\r\n",
        "        pattern = r\"(\\d{4}-\\d{2}-\\d{2}) (\\d{2}:\\d{2}:\\d{2})\"\r\n",
        "\r\n",
        "        destination = spark.sql(\"SELECT COALESCE(MAX(date_time), CAST('1999-01-01' AS TIMESTAMP)) AS max_date_time FROM Cricket_Analysis_Data\").collect()\r\n",
        "        max_date_time = destination[0][\"max_date_time\"] if destination and destination[0][\"max_date_time\"] else None\r\n",
        "\r\n",
        "        # looping each file in the directory \r\n",
        "        for file_info in files:\r\n",
        "            file_path = file_info.path\r\n",
        "            file_name = file_info.name      \r\n",
        "            match = re.search(pattern, file_name)\r\n",
        "            \r\n",
        "            if match:\r\n",
        "                date_time = f\"{match.group(1)} {match.group(2)}\"\r\n",
        "            \r\n",
        "                df = spark.createDataFrame([(file_name, date_time)], [\"file_name\", \"date_time\"])\r\n",
        "                df = df.withColumn(\"date_time\", to_timestamp(\"date_time\", \"yyyy-MM-dd HH:mm:ss\"))\r\n",
        "\r\n",
        "                if max_date_time:\r\n",
        "                    if df.collect()[0][\"date_time\"] > max_date_time:\r\n",
        "                        print(f\"Source date_time ({df.collect()[0]['date_time']}) is newer. Proceed with processing {file_name}.\")\r\n",
        "\r\n",
        "                        path = f\"abfss://bronzelayer@gen2storageforazurepoc.dfs.core.windows.net/{file_name}\"\r\n",
        "                        print(path)\r\n",
        "                    \r\n",
        "                        stage_df = spark.read.option(\"header\",\"true\").schema(source_schema).option(\"mode\", \"PERMISSIVE\").option(\"columnNameOfCorruptRecord\", \"_corrupt_record\").csv(path)\r\n",
        "                        #display(stage_df)\r\n",
        "                        \r\n",
        "                        stage_df.cache()\r\n",
        "\r\n",
        "                        cn_df = stage_df.where(col(\"_corrupt_record\").isNotNull()).count()\r\n",
        "\r\n",
        "                        if cn_df>0:\r\n",
        "\r\n",
        "                            #Handling Corrupted Records...\r\n",
        "                            print(\"Corrupted Records Found. Started loading into corruptedrecords container and also in process_corrupt_record table... \")\r\n",
        "\r\n",
        "                            currupted_records= stage_df.where(col(\"_corrupt_record\").isNotNull())\r\n",
        "                            currupted_records.write.option(\"header\",True).mode(\"append\").csv(\"abfss://corruptedrecords@gen2storageforazurepoc.dfs.core.windows.net/corrupted records/\")\r\n",
        "\r\n",
        "                            currupted_records_df = stage_df \\\r\n",
        "                                .withColumn(\"Pipeline_Id\", lit(Pipeline_Id)) \\\r\n",
        "                                .withColumn(\"Pipeline_Name\", lit(Pipeline_Name)) \\\r\n",
        "                                .withColumn(\"file_name\", lit(file_name)) \\\r\n",
        "                                .withColumn(\"path\", lit(path)) \\\r\n",
        "                                .withColumn(\"date_time\", to_timestamp(current_timestamp(), \"yyyy-MM-dd HH:mm:ss\")) \\\r\n",
        "                                .select(\"Pipeline_Id\", \"Pipeline_Name\", \"file_name\", \"path\", \"_corrupt_record\", \"date_time\").where(col(\"_corrupt_record\").isNotNull())\r\n",
        "\r\n",
        "                            currupted_records_df.write.jdbc(url=jdbc_url, table=\"logs.process_corrupt_record\", mode=\"append\", properties=connection_properties)\r\n",
        "                            print(\"Corrupted records loaded into a storage and processed_corrupt_record table successfully\")\r\n",
        "                        else:\r\n",
        "                            print(\"No corrupted Records found\")\r\n",
        "\r\n",
        "                        # Processing Non corrupted records\r\n",
        "                        print(\"Processing Non corrupted records.......\")\r\n",
        "                        source_df = stage_df.where(col(\"_corrupt_record\").isNull()).drop(\"_corrupt_record\")\r\n",
        "                        display(source_df)\r\n",
        "\r\n",
        "                        #extracting the date time from the file\r\n",
        "                        file_path = f\"{path}\"\r\n",
        "                        pattern = r\"(\\d{4}-\\d{2}-\\d{2}) (\\d{2}:\\d{2}:\\d{2})\"\r\n",
        "                        match = re.search(pattern, file_path)\r\n",
        "\r\n",
        "                        if match:\r\n",
        "                            date_time = f\"{match.group(1)} {match.group(2)}\"\r\n",
        "                            print(date_time)\r\n",
        "                        \r\n",
        "                        else:\r\n",
        "                            print(\"No date and time found in the file name.\")\r\n",
        "\r\n",
        "\r\n",
        "                        # working on transformations\r\n",
        "                        transforming_source_df = source_df.withColumn(\"batter\", regexp_replace(col(\"batter\"), \"[^a-zA-Z0-9- ]\", \"\"))\\\r\n",
        "                                            .withColumn(\"bowler\", regexp_replace(col(\"bowler\"), \"[^a-zA-Z0-9- ]\", \"\"))\\\r\n",
        "                                            .withColumn(\"non_striker\", regexp_replace(col(\"non_striker\"), \"[^a-zA-Z0-9- ]\", \"\"))\\\r\n",
        "                                            .withColumn(\"player_out\", regexp_replace(col(\"player_out\"), \"-\", \"\"))\\\r\n",
        "                                            .fillna({\"runs_batter\": 0, \"runs_extras\": 0,\"runs_total\": 0})\\\r\n",
        "                                            .withColumn(\"match_id\", substring_index(col(\"match_id\"), \".\", 1))\\\r\n",
        "                                            .withColumn(\"date_time\", to_timestamp(lit(date_time), \"yyyy-MM-dd HH:mm:ss\"))\r\n",
        "\r\n",
        "                        #Generating hash key\r\n",
        "                        df_with_hash = transforming_source_df.withColumn(\r\n",
        "                            \"hash_key\", \r\n",
        "                            md5(\r\n",
        "                                concat_ws(\r\n",
        "                                    \"_\", \r\n",
        "                                    col(\"batter\"),\r\n",
        "                                    col(\"bowler\"),\r\n",
        "                                    col(\"non_striker\"),\r\n",
        "                                    col(\"runs_batter\"),\r\n",
        "                                    col(\"runs_extras\"),\r\n",
        "                                    col(\"runs_total\"),\r\n",
        "                                    col(\"delivery_number\"),\r\n",
        "                                    col(\"over_number\"),\r\n",
        "                                    col(\"inning_number\"),\r\n",
        "                                    col(\"match_name\"),\r\n",
        "                                    col(\"match_id\"),\r\n",
        "                                    col(\"match_city\"),\r\n",
        "                                    col(\"match_venue\"),\r\n",
        "                                    col(\"match_type\"),\r\n",
        "                                    col(\"team_type\"),\r\n",
        "                                    col(\"match_start_date\"),\r\n",
        "                                    col(\"player_out\")\r\n",
        "                                )\r\n",
        "                            )\r\n",
        "                        )\r\n",
        "                        \r\n",
        "                        #display(df_with_hash)\r\n",
        "                        print(\"Hash code Generated\")\r\n",
        "\r\n",
        "                        df_with_hash.createOrReplaceGlobalTempView(\"df_with_hash_table\")\r\n",
        "                        print(\"Hash dataframe converted to temp table\")\r\n",
        "\r\n",
        "                        spark.sql(f\"\"\"MERGE INTO {dest_table_name} AS target\r\n",
        "                        USING global_temp.df_with_hash_table AS source\r\n",
        "                        ON target.hash_key = source.hash_key\r\n",
        "                        WHEN NOT MATCHED THEN\r\n",
        "                            INSERT (id, batter, bowler, non_striker, runs_batter, runs_extras, runs_total, delivery_number, over_number, team, inning_number, match_name, match_id, match_city, match_venue, match_type, team_type, match_start_date, player_out,date_time,hash_key)\r\n",
        "                            VALUES (source.id, source.batter, source.bowler, source.non_striker, source.runs_batter, source.runs_extras, source.runs_total, source.delivery_number, source.over_number, source.team, source.inning_number, source.match_name, source.match_id, source.match_city, source.match_venue, source.match_type, source.team_type, source.match_start_date, source.player_out,source.date_time,source.hash_key);\r\n",
        "                        \"\"\")\r\n",
        "                        print(f\"Inserted data into {dest_table_name}\")\r\n",
        "                         \r\n",
        "                    else:\r\n",
        "                        print(\"No need to process. Already processed Data\")\r\n",
        "                else:\r\n",
        "                    print(\"No need to process\")\r\n",
        "            else:\r\n",
        "                print(f\"No date and time found in the file name: {file_name}\")\r\n",
        "            "
      ],
      "outputs": [],
      "execution_count": 87,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dest_database_name = \"default\"\r\n",
        "dest_schema_name = \"dbo\"\r\n",
        "dest_table_name = \"Cricket_Analysis_Data\"\r\n",
        "\r\n",
        "result = spark.sql(f\"SHOW TABLES IN {dest_database_name}\").filter(f\"tableName = '{dest_table_name}'\").count()\r\n",
        "\r\n",
        "if result > 0:\r\n",
        "    process_files()\r\n",
        "else:\r\n",
        "    schema = StructType([\r\n",
        "                            StructField(\"id\", IntegerType(), True),\r\n",
        "                            StructField(\"batter\", StringType(), True),\r\n",
        "                            StructField(\"bowler\", StringType(), True),\r\n",
        "                            StructField(\"non_striker\", StringType(), True),\r\n",
        "                            StructField(\"runs_batter\", IntegerType(), True),\r\n",
        "                            StructField(\"runs_extras\", IntegerType(), True),\r\n",
        "                            StructField(\"runs_total\", IntegerType(), True),\r\n",
        "                            StructField(\"delivery_number\", IntegerType(), True),\r\n",
        "                            StructField(\"over_number\", IntegerType(), True),  \r\n",
        "                            StructField(\"team\", StringType(), True),\r\n",
        "                            StructField(\"inning_number\", IntegerType(), True),\r\n",
        "                            StructField(\"match_name\", StringType(), True),\r\n",
        "                            StructField(\"match_id\", StringType(), True),\r\n",
        "                            StructField(\"match_city\", StringType(), True),\r\n",
        "                            StructField(\"match_venue\", StringType(), True),\r\n",
        "                            StructField(\"match_type\", StringType(), True),\r\n",
        "                            StructField(\"team_type\", StringType(), True),\r\n",
        "                            StructField(\"match_start_date\", DateType(), True),\r\n",
        "                            StructField(\"player_out\", StringType(), True),\r\n",
        "                            StructField(\"date_time\", TimestampType(), True),\r\n",
        "                            StructField(\"hash_key\", StringType(), True),\r\n",
        "                        ])\r\n",
        "\r\n",
        "    lake_df = spark.createDataFrame([], schema)\r\n",
        "    lake_df.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{dest_table_name}\")\r\n",
        "    process_files()\r\n"
      ],
      "outputs": [],
      "execution_count": 89,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jdbc_url = \"jdbc:sqlserver://sqlserver8341.database.windows.net:1433;database=GoldLayer\"\r\n",
        "connection_properties = {\r\n",
        "    \"user\": \"suneetha\",\r\n",
        "    \"password\": \"Suni@123\",\r\n",
        "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
        "}"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool",
              "statement_id": 3,
              "statement_ids": [
                3
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T12:41:58.554509Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T12:41:58.7257865Z",
              "execution_finish_time": "2024-12-01T12:41:58.8887713Z",
              "parent_msg_id": "5ab055f9-7413-43e3-bc92-d8d725d940b9"
            },
            "text/plain": "StatementMeta(sparkpool, 1, 3, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Gold_Team_Read = spark.read.jdbc(url=jdbc_url, table=\"gold.gl_Team\",  properties=connection_properties)\r\n",
        "Gold_Team_Read.show()"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Team info</b>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "\r\n",
        "# Load the data\r\n",
        "df = spark.sql(\"\"\"\r\n",
        "    SELECT Team_Name, Team_Type \r\n",
        "        FROM (\r\n",
        "        SELECT DISTINCT team AS Team_Name, team_type AS Team_Type, COUNT(*) AS con\r\n",
        "        FROM cricket_analysis_data\r\n",
        "        GROUP BY team, team_type\r\n",
        "        ) AS teams_dataset\r\n",
        "\"\"\")\r\n",
        "\r\n",
        "Team_df_with_id = df.withColumn(\"Team_ID\", F.abs(F.hash(F.col(\"Team_Name\"), F.col(\"Team_Type\"))))\r\n",
        "\r\n",
        "Team_df = Team_df_with_id.select(\"Team_ID\", \"Team_Name\", \"Team_Type\").distinct()\r\n",
        "\r\n",
        "Gold_Team_Read = spark.read.jdbc(url=jdbc_url, table=\"[gold].[Team_Dim]\",  properties=connection_properties)\r\n",
        "Gold_Team_Read\r\n",
        "\r\n",
        "Gold_Team_Join_df = Team_df.join(Gold_Team_Read, Team_df[\"Team_ID\"] == Gold_Team_Read[\"Team_ID\"],\"left_anti\")\r\n",
        "Gold_Team_Join_df.show()\r\n",
        "\r\n",
        "Gold_Team_Join_df.write.jdbc(url=jdbc_url, table=\"[gold].[Team_Dim]\", mode=\"append\", properties=connection_properties)\r\n",
        "print(\"Loaded successfully\")\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool",
              "statement_id": 4,
              "statement_ids": [
                4
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T12:42:02.7143104Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T12:42:02.8953287Z",
              "execution_finish_time": "2024-12-01T12:42:26.4072079Z",
              "parent_msg_id": "66d2554e-903e-4b14-bb85-b3045f990360"
            },
            "text/plain": "StatementMeta(sparkpool, 1, 4, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+---------+\n|Team_ID|Team_Name|Team_Type|\n+-------+---------+---------+\n+-------+---------+---------+\n\nLoaded successfully\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>player info</b>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "df = spark.sql(\"\"\"\r\n",
        "     SELECT DISTINCT player_name\r\n",
        "        FROM (\r\n",
        "            SELECT DISTINCT batter AS player_name FROM cricket_analysis_data\r\n",
        "            UNION\r\n",
        "            SELECT DISTINCT bowler AS player_name FROM cricket_analysis_data\r\n",
        "        ) AS player_data\r\n",
        "\"\"\")\r\n",
        "\r\n",
        "player_df_with_id = df.withColumn(\"Player_ID\", F.abs(F.hash(F.col(\"player_name\"))))\r\n",
        "\r\n",
        "players_df = player_df_with_id.select(\"Player_ID\", \"Player_Name\").distinct()\r\n",
        "\r\n",
        "#players_df.show()\r\n",
        "\r\n",
        "Player_Gold_Read = spark.read.jdbc(url=jdbc_url, table=\"[gold].[Player_Dim]\",  properties=connection_properties)\r\n",
        "#Player_Gold_Read.show()\r\n",
        "\r\n",
        "Player_Gold_Join_df = players_df.join(Player_Gold_Read, players_df[\"Player_ID\"] == Player_Gold_Read[\"Player_ID\"],\"left_anti\")\r\n",
        "Player_Gold_Join_df.show()\r\n",
        "\r\n",
        "Player_Gold_Join_df.write.jdbc(url=jdbc_url, table=\"[gold].[Player_Dim]\", mode=\"append\", properties=connection_properties)\r\n",
        "print(\"Loaded successfully\")\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool",
              "statement_id": 5,
              "statement_ids": [
                5
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T12:42:30.1188133Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T12:42:30.2588094Z",
              "execution_finish_time": "2024-12-01T12:42:34.4128062Z",
              "parent_msg_id": "2f2e3977-55c1-4ddf-b336-378037b1de1c"
            },
            "text/plain": "StatementMeta(sparkpool, 1, 5, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+\n|Player_ID|Player_Name|\n+---------+-----------+\n+---------+-----------+\n\nLoaded successfully\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> relational table for players and team</b>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "df = spark.sql(\"\"\"\r\n",
        "SELECT data.match_venue AS Match_Venue\r\n",
        "\t,data.match_city AS Match_City\r\n",
        "FROM (\r\n",
        "\tSELECT DISTINCT match_venue\r\n",
        "\t\t,match_city\r\n",
        "\t\t,row_number() OVER (\r\n",
        "\t\t\tPARTITION BY match_venue ORDER BY match_city DESC\r\n",
        "\t\t\t) AS rownum\r\n",
        "\tFROM stage_match_details\r\n",
        "    Where Match_Venue IS NOT NULL\r\n",
        "\t) AS data\r\n",
        "WHERE rownum = 1\r\n",
        "\"\"\")\r\n",
        "\r\n",
        "Location_df_with_id = df.withColumn(\"Location_ID\", F.abs(F.hash(F.col(\"match_venue\"))))\r\n",
        "\r\n",
        "Location_df = Location_df_with_id.select(\"Location_ID\", \"Match_Venue\",\"Match_City\").distinct()\r\n",
        "\r\n",
        "\r\n",
        "# Reading data from the gold layer from location table\r\n",
        "\r\n",
        "Location_Gold_Read = spark.read.jdbc(url=jdbc_url, table=\"[gold].[Location_Dim]\",  properties=connection_properties)\r\n",
        "Location_Gold_Read.show()\r\n",
        "\r\n",
        "\r\n",
        "Location_Gold_Join_df = Location_df.join(Location_Gold_Read, Location_df[\"Location_ID\"] == Location_Gold_Read[\"Location_ID\"],\"left_anti\")\r\n",
        "Location_Gold_Join_df.show()\r\n",
        "\r\n",
        "Location_Gold_Join_df.write.jdbc(url=jdbc_url, table=\"[gold].[Location_Dim]\", mode=\"append\", properties=connection_properties)\r\n",
        "print(\"Loaded successfully\")\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool",
              "statement_id": 6,
              "statement_ids": [
                6
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T12:42:40.8337014Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T12:42:40.9674354Z",
              "execution_finish_time": "2024-12-01T12:42:48.0056318Z",
              "parent_msg_id": "1fe82ee8-ee97-4999-b4d9-ec3e2038c5e1"
            },
            "text/plain": "StatementMeta(sparkpool, 1, 6, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------------+--------------+\n|Location_id|         Match_Venue|    Match_City|\n+-----------+--------------------+--------------+\n|   16343596|   Marsa Sports Club|         Marsa|\n|   16899714|    St George's Park|Port Elizabeth|\n|   28488780|         Manuka Oval|      Canberra|\n|   47638255|        Ashlyns Road|Frinton-on-Sea|\n|   54692628|Zahur Ahmed Chowd...|    Chittagong|\n|   60546042|       Nehru Stadium|         Kochi|\n|   61686815|County Ground, Derby|         Derby|\n|   63718776|Rangiri Dambulla ...|          null|\n|   66836724|Willowmoore Park ...|        Benoni|\n|   85835543|King George V Spo...|        Castel|\n|   93736385|St Lawrence Groun...|    Canterbury|\n|   97112906|Maple Leaf North-...|     King City|\n|  127138329|Saurashtra Cricke...|        Rajkot|\n|  156973994|Pallekele Interna...|          null|\n|  166891998|Subrata Roy Sahar...|          Pune|\n|  167640842|         Seddon Park|      Hamilton|\n|  182699840|Warner Park, Bass...|      St Kitts|\n|  195917853|       Albert Park 2|          Suva|\n|  219241599|       County Ground|   Northampton|\n|  232219334|Old Trafford, Man...|    Manchester|\n+-----------+--------------------+--------------+\nonly showing top 20 rows\n\n+-----------+-----------+----------+\n|Location_ID|Match_Venue|Match_City|\n+-----------+-----------+----------+\n+-----------+-----------+----------+\n\nLoaded successfully\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "df = spark.sql(\"\"\"\r\n",
        "SELECT ca.match_id as Match_Id\r\n",
        "\t,ca.match_name as Match_Name\r\n",
        "\t,match_type as Match_Type\r\n",
        "\t,ca.Match_venue as Match_Venue\r\n",
        "\t,inning_number as Innings_Number\r\n",
        "\t,match_start_date as Match_Start_Date\r\n",
        "FROM cricket_analysis_data ca\r\n",
        "JOIN stage_Match_Location b ON ca.match_venue = b.match_venue\r\n",
        "\r\n",
        "\"\"\")\r\n",
        "\r\n",
        "Join_match_location = df.join(Location_df, df.Match_Venue == Location_df.Match_Venue,\"inner\")\r\n",
        "\r\n",
        "\r\n",
        "Match_result = Join_match_location.select(\r\n",
        "    df[\"Match_Id\"],\r\n",
        "    df[\"Match_Name\"],\r\n",
        "    df[\"Match_Type\"],\r\n",
        "\tdf[\"Innings_Number\"],\r\n",
        "\tLocation_df[\"Location_ID\"],\r\n",
        "    df[\"Match_Start_Date\"]\r\n",
        ").distinct()\r\n",
        "\r\n",
        "\r\n",
        "Match_Gold_Read = spark.read.jdbc(url=jdbc_url, table=\"[Gold].[Match_Fact]\",  properties=connection_properties)\r\n",
        "Match_Gold_Read.show()\r\n",
        "\r\n",
        "Match_Gold_Join_df = Match_result.join(Match_Gold_Read, Match_result[\"Match_Id\"] == Match_Gold_Read[\"Match_Id\"],\"left_anti\")\r\n",
        "Match_Gold_Join_df.show()\r\n",
        "\r\n",
        "Match_Gold_Join_df.write.jdbc(url=jdbc_url, table=\"[Gold].[Match_Fact]\", mode=\"append\", properties=connection_properties)\r\n",
        "\r\n",
        "print(\"Loaded successfully\")\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool",
              "statement_id": 58,
              "statement_ids": [
                58
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T17:56:46.5487904Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T17:56:46.7333289Z",
              "execution_finish_time": "2024-12-01T17:56:49.6922994Z",
              "parent_msg_id": "00158042-29ac-41ec-873f-593b96ce715e"
            },
            "text/plain": "StatementMeta(sparkpool, 1, 58, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+----------+--------------+-----------+----------------+\n|Match_Id|Match_Name|Match_Type|Innings_Number|Location_id|Match_Start_Date|\n+--------+----------+----------+--------------+-----------+----------------+\n+--------+----------+----------+--------------+-----------+----------------+\n\n+--------+--------------------+----------+--------------+-----------+----------------+\n|Match_Id|          Match_Name|Match_Type|Innings_Number|Location_ID|Match_Start_Date|\n+--------+--------------------+----------+--------------+-----------+----------------+\n| 1166914|Specsavers County...|       MDM|             2| 1099665471|      2019-04-05|\n| 1094702|Caribbean Premier...|       T20|             1|  649402276|      2017-09-05|\n|  576416|ICC World Cricket...|       ODM|             1| 2042149949|      2012-09-09|\n|  693029|   NatWest T20 Blast|       T20|             2| 2140043680|      2014-05-18|\n|  249749|ICC Champions Trophy|       ODI|             1| 1768679875|      2006-10-20|\n| 1244512|    Sheffield Shield|       MDM|             2|  374742700|      2021-02-17|\n| 1257946|Nepal Tri-Nation ...|       T20|             1|  329116842|      2021-04-18|\n|  667651|India in New Zeal...|      Test|             2|  797213209|      2014-02-06|\n|  223334|Sri Lanka tour of...|       ODI|             2|  513196035|      2005-10-28|\n|  518071|ICC World Cricket...|       ODM|             2|  703199452|      2011-07-25|\n|  958419|Walton T20 Cricke...|       T20|             1|  304761173|      2016-01-20|\n| 1195621|     Big Bash League|       T20|             1| 1185405995|      2020-01-23|\n|  734009|Indian Premier Le...|       T20|             1| 1778907377|      2014-05-14|\n|  335985|Indian Premier Le...|       T20|             2| 1601040522|      2008-04-20|\n|  325579|European Champion...|       ODI|             2| 1014839798|      2008-07-29|\n|  598037|Indian Premier Le...|       T20|             2| 1601040522|      2013-04-29|\n| 1122283|India tour of Sou...|       ODI|             1|   16899714|      2018-02-13|\n| 1082624|Indian Premier Le...|       T20|             2| 1706399957|      2017-04-29|\n|  829805|Indian Premier Le...|       T20|             2| 1601040522|      2015-05-14|\n|  946865|Specsavers County...|       MDM|             3| 1220993670|      2016-06-20|\n+--------+--------------------+----------+--------------+-----------+----------------+\nonly showing top 20 rows\n\nLoaded successfully\n"
          ]
        }
      ],
      "execution_count": 57,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Match Info</b>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "df = spark.sql(\"\"\"\r\n",
        "     select DISTINCT player_name,team  FROM (\r\n",
        "    select distinct batter as player_name,team from cricket_analysis_data\r\n",
        "    UNION\r\n",
        "    select distinct bowler as player_name,team from cricket_analysis_data\r\n",
        "    ) as player_data \r\n",
        "\"\"\")\r\n",
        "\r\n",
        "\r\n",
        "player_join_df = df.join(Team_df, df.team == Team_df.Team_Name,\"inner\") \\\r\n",
        "                 .join(players_df, df.player_name == players_df.Player_Name,\"inner\")\r\n",
        "\r\n",
        "\r\n",
        "player_team_df = player_join_df.select(\"Player_ID\", \"Team_ID\").distinct()\r\n",
        "\r\n",
        "TeamPlay_Gold_Read = spark.read.jdbc(url=jdbc_url, table=\"[Gold].[Player_Team_Relational_dim]\",  properties=connection_properties)\r\n",
        "\r\n",
        "#TeamMatch_Gold_Read.show()\r\n",
        "\r\n",
        "\r\n",
        "TeamPlay_Gold_Join_df = player_team_df.join(TeamPlay_Gold_Read, (player_team_df[\"Team_ID\"] == TeamPlay_Gold_Read[\"Team_ID\"]) & (player_team_df[\"Player_ID\"] == TeamPlay_Gold_Read[\"Player_ID\"]),\"left_anti\")\r\n",
        "\r\n",
        "TeamPlay_Gold_Join_df.show()\r\n",
        "\r\n",
        "TeamPlay_Gold_Join_df.write.jdbc(url=jdbc_url, table=\"[Gold].[Player_Team_Relational_dim]\", mode=\"append\", properties=connection_properties)\r\n",
        "\r\n",
        "print(\"Loaded successfully\")\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool",
              "statement_id": 24,
              "statement_ids": [
                24
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T16:37:01.8126694Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T16:37:02.0081703Z",
              "execution_finish_time": "2024-12-01T16:37:07.4886512Z",
              "parent_msg_id": "07d42112-e7ba-4bb7-b965-ed68128833d3"
            },
            "text/plain": "StatementMeta(sparkpool, 1, 24, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n| Player_ID|   Team_ID|\n+----------+----------+\n| 577697921|1535997519|\n|1628105426|1216931668|\n| 860565847|1029878132|\n|2066164182| 424095793|\n| 931680392| 961597003|\n| 862028888|1504815257|\n|1385849740| 993809512|\n|1841348667|1860782890|\n| 362721381|1296592871|\n|1649245999| 318216826|\n|1498266766|1918973500|\n|1261873785| 424095793|\n| 950899226|1830915450|\n|1611647657| 498005623|\n|  10940854|1698303436|\n|1508379357|1161712431|\n|1890435699|1995353329|\n|1233518762|1201572248|\n| 474328113| 593509167|\n|2013569934| 724494422|\n+----------+----------+\nonly showing top 20 rows\n\nLoaded successfully\n"
          ]
        }
      ],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "df = spark.sql(\"\"\"\r\n",
        "SELECT match_id,Inning_Number as Innings_Number,team FROM cricket_analysis_data \r\n",
        "\"\"\")\r\n",
        "\r\n",
        "\r\n",
        "Match_team_join_df = df.join(Team_df, df.team == Team_df.Team_Name,\"inner\")\r\n",
        "#Match_team_join_df.show()\r\n",
        "\r\n",
        "Match_Team_result = Match_team_join_df.select(\r\n",
        "    df[\"Match_Id\"],\r\n",
        "    df[\"Innings_Number\"],\r\n",
        "    Team_df[\"Team_ID\"]\r\n",
        ")\r\n",
        "\r\n",
        "#Match_Team_result.show()\r\n",
        "\r\n",
        "TeamMatch_Gold_Read = spark.read.jdbc(url=jdbc_url, table=\"[Gold].[Team_Match_Relational_dim]\",  properties=connection_properties)\r\n",
        "\r\n",
        "#TeamMatch_Gold_Read.show()\r\n",
        "\r\n",
        "\r\n",
        "TeamMatch_Gold_Join_df = Match_Team_result.join(TeamMatch_Gold_Read, (Match_Team_result[\"Team_ID\"] == TeamMatch_Gold_Read[\"Team_ID\"]) & (Match_Team_result[\"Match_ID\"] == TeamMatch_Gold_Read[\"Match_ID\"]),\"left_anti\")\r\n",
        "\r\n",
        "TeamMatch_Gold_Join_df.show()\r\n",
        "\r\n",
        "TeamMatch_Gold_Join_df.write.jdbc(url=jdbc_url, table=\"[Gold].[Team_Match_Relational_dim]\", mode=\"append\", properties=connection_properties)\r\n",
        "\r\n",
        "print(\"Loaded successfully\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool",
              "statement_id": 62,
              "statement_ids": [
                62
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T17:58:56.4930966Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T17:58:56.6301348Z",
              "execution_finish_time": "2024-12-01T18:00:56.7451026Z",
              "parent_msg_id": "5b2c4ef6-0075-420e-b89f-8d5dbb716975"
            },
            "text/plain": "StatementMeta(sparkpool, 1, 62, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------+---------+\n|Match_Id|Innings_Number|  Team_ID|\n+--------+--------------+---------+\n| 1023591|             1|821019452|\n| 1023591|             1|821019452|\n| 1023591|             1|821019452|\n| 1023591|             1|821019452|\n| 1023591|             1|821019452|\n| 1023591|             1|821019452|\n| 1023591|             1|821019452|\n| 1023591|             1|821019452|\n| 1023591|             1|821019452|\n| 1023591|             1|821019452|\n| 1023591|             1|821019452|\n| 1023591|             1|821019452|\n| 1023591|             1|821019452|\n| 1023591|             1|821019452|\n| 1023591|             1|821019452|\n| 1023591|             1|821019452|\n| 1023591|             1|821019452|\n| 1023591|             1|821019452|\n| 1023591|             1|821019452|\n| 1023591|             1|821019452|\n+--------+--------------+---------+\nonly showing top 20 rows\n\nLoaded successfully\n"
          ]
        }
      ],
      "execution_count": 61,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Creating Fact Table </b>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batter_fact = spark.sql('''\r\n",
        "WITH aggregated_data AS (\r\n",
        "    SELECT \r\n",
        "        batter,\r\n",
        "        Match_Id,\r\n",
        "\t\tInning_Number,\r\n",
        "        SUM(Runs_Batter) AS sum_batter_runs,\r\n",
        "        SUM(Runs_Total) AS total_score,\r\n",
        "        COUNT(CASE WHEN runs_batter = 4 THEN 1 END) AS count_of_fours,\r\n",
        "        COUNT(CASE WHEN runs_batter = 6 THEN 1 END) AS count_of_sixs,\r\n",
        "\t\tCOUNT(CASE WHEN player_out is not null THEN 1 END) AS player_out\r\n",
        "\t    ,case when sum(Runs_Total) = 0 then 1 else 0 end duck_out\r\n",
        "\r\n",
        "    FROM \r\n",
        "        cricket_analysis_data\r\n",
        "    GROUP BY \r\n",
        "        batter, Match_Id,Inning_Number\r\n",
        ")\r\n",
        "SELECT \r\n",
        "    batter,\r\n",
        "    sum_batter_runs,\r\n",
        "    total_score,\r\n",
        "    COALESCE(count_of_fours, 0) AS count_of_fours,\r\n",
        "    COALESCE(count_of_sixs, 0) AS count_of_sixs,\r\n",
        "\tCOALESCE(player_out, 0) AS player_out,\r\n",
        "\tduck_out\r\n",
        "\t,Match_Id\r\n",
        "\t,Inning_Number as Innings_Number\r\n",
        "FROM aggregated_data\r\n",
        "\r\n",
        "''')\r\n",
        "\r\n",
        "#batter_fact.show()\r\n",
        "\r\n",
        "batter_fact_join = batter_fact.join(players_df, players_df.Player_Name == batter_fact.batter,\"inner\")\r\n",
        "#batter_fact_join.show()\r\n",
        "\r\n",
        "rr = batter_fact_join.withColumnRenamed(\"player_ID\",\"Batter_ID\")\r\n",
        "\r\n",
        "result = rr.select(\"Batter_ID\",\"Match_Id\",\"Innings_Number\",\"sum_batter_runs\",\"total_score\",\"count_of_fours\",\"count_of_sixs\",\"player_out\",\"duck_out\").distinct()\r\n",
        "result.show()\r\n",
        "\r\n",
        "batterfact_Gold_Read = spark.read.jdbc(url=jdbc_url, table=\"[Gold].[Batter_Fact]\",  properties=connection_properties)\r\n",
        "#batterfact_Gold_Read.show()\r\n",
        "\r\n",
        "batter_fact_Gold_Join_df = result.join(batterfact_Gold_Read, (result[\"Batter_ID\"] == batterfact_Gold_Read[\"Batter_ID\"]) & (result[\"Match_ID\"] == batterfact_Gold_Read[\"Match_ID\"]),\"left_anti\")\r\n",
        "\r\n",
        "batter_fact_Gold_Join_df.show()\r\n",
        "\r\n",
        "batter_fact_Gold_Join_df.write.jdbc(url=jdbc_url, table=\"[Gold].[Batter_Fact]\", mode=\"append\", properties=connection_properties)\r\n",
        "\r\n",
        "print(\"Loaded successfully\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool",
              "statement_id": 65,
              "statement_ids": [
                65
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T18:02:04.2425149Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T18:02:04.4435439Z",
              "execution_finish_time": "2024-12-01T18:02:09.9089819Z",
              "parent_msg_id": "21424183-ef48-41af-83fb-783fef5c52b8"
            },
            "text/plain": "StatementMeta(sparkpool, 1, 65, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+--------------+---------------+-----------+--------------+-------------+----------+--------+\n| Batter_ID|Match_Id|Innings_Number|sum_batter_runs|total_score|count_of_fours|count_of_sixs|player_out|duck_out|\n+----------+--------+--------------+---------------+-----------+--------------+-------------+----------+--------+\n|1253226073|  946825|             2|             26|         26|             2|            0|         1|       0|\n| 771742250|  531987|             3|            195|        205|            12|            5|         1|       0|\n|1602648736|  804557|             1|             20|         22|             1|            1|         1|       0|\n| 720368411| 1068642|             2|             17|         19|             2|            0|         1|       0|\n|1261873785|  225254|             2|             23|         25|             4|            0|         0|       0|\n|1347876109|  355996|             2|             20|         22|             1|            0|         1|       0|\n| 578113267|  743969|             3|             13|         18|             1|            0|         1|       0|\n|  30839110|  531629|             4|              7|          7|             0|            0|         1|       0|\n|1937436088|  386531|             1|             66|         70|             7|            1|         1|       0|\n|1980996512|  947065|             1|             32|         33|             7|            0|         1|       0|\n|1521314424|  501247|             1|              1|          1|             0|            0|         0|       0|\n|1838457418| 1130743|             2|              2|          4|             0|            0|         1|       0|\n| 760233994|   64094|             1|            167|        173|            17|            7|         1|       0|\n|2068998688| 1144163|             1|              0|          0|             0|            0|         0|       1|\n| 775206963|  875477|             2|             10|         10|             2|            0|         1|       0|\n| 132609383| 1119547|             1|             13|         13|             1|            0|         1|       0|\n|1783309506|  804557|             2|             21|         21|             1|            1|         1|       0|\n| 318816432| 1144163|             1|             22|         23|             0|            3|         0|       0|\n|1061960200| 1207714|             2|             26|         26|             2|            0|         1|       0|\n| 795578055| 1283091|             1|              8|         10|             0|            0|         1|       0|\n+----------+--------+--------------+---------------+-----------+--------------+-------------+----------+--------+\nonly showing top 20 rows\n\n+----------+--------+--------------+---------------+-----------+--------------+-------------+----------+--------+\n| Batter_ID|Match_Id|Innings_Number|sum_batter_runs|total_score|count_of_fours|count_of_sixs|player_out|duck_out|\n+----------+--------+--------------+---------------+-----------+--------------+-------------+----------+--------+\n|1253226073|  946825|             2|             26|         26|             2|            0|         1|       0|\n| 771742250|  531987|             3|            195|        205|            12|            5|         1|       0|\n|1602648736|  804557|             1|             20|         22|             1|            1|         1|       0|\n| 720368411| 1068642|             2|             17|         19|             2|            0|         1|       0|\n|1261873785|  225254|             2|             23|         25|             4|            0|         0|       0|\n|1347876109|  355996|             2|             20|         22|             1|            0|         1|       0|\n| 578113267|  743969|             3|             13|         18|             1|            0|         1|       0|\n|  30839110|  531629|             4|              7|          7|             0|            0|         1|       0|\n|1937436088|  386531|             1|             66|         70|             7|            1|         1|       0|\n|1521314424|  501247|             1|              1|          1|             0|            0|         0|       0|\n|1980996512|  947065|             1|             32|         33|             7|            0|         1|       0|\n|1838457418| 1130743|             2|              2|          4|             0|            0|         1|       0|\n| 760233994|   64094|             1|            167|        173|            17|            7|         1|       0|\n|2068998688| 1144163|             1|              0|          0|             0|            0|         0|       1|\n| 775206963|  875477|             2|             10|         10|             2|            0|         1|       0|\n| 132609383| 1119547|             1|             13|         13|             1|            0|         1|       0|\n|1783309506|  804557|             2|             21|         21|             1|            1|         1|       0|\n| 318816432| 1144163|             1|             22|         23|             0|            3|         0|       0|\n|1061960200| 1207714|             2|             26|         26|             2|            0|         1|       0|\n| 795578055| 1283091|             1|              8|         10|             0|            0|         1|       0|\n+----------+--------+--------------+---------------+-----------+--------------+-------------+----------+--------+\nonly showing top 20 rows\n\nLoaded successfully\n"
          ]
        }
      ],
      "execution_count": 64,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bowler_df = spark.sql('''\r\n",
        "WITH aggregated_data AS (\r\n",
        "    SELECT \r\n",
        "        bowler,\r\n",
        "        Match_Id,\r\n",
        "\t\tInning_Number,\r\n",
        "        SUM(Runs_Extras) AS run_extras,\r\n",
        "        COUNT(CASE WHEN Player_Out is not null THEN 1 END) AS wicket\r\n",
        "    FROM \r\n",
        "        cricket_analysis_data\r\n",
        "    GROUP BY \r\n",
        "        bowler, Match_Id,Inning_Number\r\n",
        ")\r\n",
        "SELECT \r\n",
        "    bowler,\r\n",
        "    Match_Id,\r\n",
        "    Inning_Number as Innings_Number,\r\n",
        "    run_extras as sum_of_run_extras,\r\n",
        "    COALESCE(wicket, 0) AS wicket\r\n",
        "FROM aggregated_data\r\n",
        "\r\n",
        "''')\r\n",
        "\r\n",
        "\r\n",
        "bowler_fact_join = bowler_df.join(players_df, players_df.Player_Name == bowler_df.bowler,\"inner\")\r\n",
        "#batter_fact_join.show()\r\n",
        "\r\n",
        "rr = bowler_fact_join.withColumnRenamed(\"player_ID\",\"Bowler_ID\")\r\n",
        "\r\n",
        "result = rr.select(\"Bowler_ID\",\"Match_Id\",\"Innings_Number\",\"sum_of_run_extras\",\"wicket\").distinct()\r\n",
        "result.show()\r\n",
        "\r\n",
        "Bowlerfact_Gold_Read = spark.read.jdbc(url=jdbc_url, table=\"[Gold].[Bowler_Fact]\",  properties=connection_properties)\r\n",
        "#batterfact_Gold_Read.show()\r\n",
        "\r\n",
        "bowler_fact_Gold_Join_df = result.join(Bowlerfact_Gold_Read, (result[\"Bowler_ID\"] == Bowlerfact_Gold_Read[\"Bowler_ID\"]) & (result[\"Match_ID\"] == Bowlerfact_Gold_Read[\"Match_ID\"]),\"left_anti\")\r\n",
        "\r\n",
        "bowler_fact_Gold_Join_df.show()\r\n",
        "\r\n",
        "bowler_fact_Gold_Join_df.write.jdbc(url=jdbc_url, table=\"[Gold].[Bowler_Fact]\", mode=\"append\", properties=connection_properties)\r\n",
        "\r\n",
        "print(\"Loaded successfully\")\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool",
              "statement_id": 70,
              "statement_ids": [
                70
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T18:07:23.0615423Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T18:07:23.2408311Z",
              "execution_finish_time": "2024-12-01T18:07:28.8526971Z",
              "parent_msg_id": "f72a3e99-5651-47ae-b2b0-5182965c609e"
            },
            "text/plain": "StatementMeta(sparkpool, 1, 70, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+--------------+-----------------+------+\n| Bowler_ID|Match_Id|Innings_Number|sum_of_run_extras|wicket|\n+----------+--------+--------------+-----------------+------+\n| 392674578| 1167146|             1|                0|     3|\n|1127955263|  455237|             2|                3|     4|\n|1667068185|  267710|             1|                2|     2|\n| 510427160|  433570|             1|                0|     1|\n|1820583710| 1114865|             1|                0|     1|\n|1448751226|  875477|             2|                1|     4|\n|1879849745|  692759|             1|                0|     0|\n| 128119192| 1197142|             1|                2|     0|\n| 890680799|  478279|             1|                1|     3|\n|2015376879|  576416|             1|               12|     4|\n|1361316829| 1254058|             1|                0|     0|\n|1859946190|  531629|             2|                4|     3|\n|1381923431| 1166965|             2|                6|     1|\n| 425461883|  386531|             2|                0|     1|\n| 598139433| 1207776|             1|                1|     1|\n|1552048840| 1127708|             3|                0|     0|\n| 286000548| 1118878|             2|                0|     0|\n| 785897359|  456662|             2|                5|     4|\n| 543106372|  656427|             1|                1|     0|\n|1743223352|  756757|             1|                1|     2|\n+----------+--------+--------------+-----------------+------+\nonly showing top 20 rows\n\n+----------+--------+--------------+-----------------+------+\n| Bowler_ID|Match_Id|Innings_Number|sum_of_run_extras|wicket|\n+----------+--------+--------------+-----------------+------+\n| 392674578| 1167146|             1|                0|     3|\n|1127955263|  455237|             2|                3|     4|\n|  46956297| 1144991|             1|                5|     2|\n|1667068185|  267710|             1|                2|     2|\n| 510427160|  433570|             1|                0|     1|\n|1820583710| 1114865|             1|                0|     1|\n|1448751226|  875477|             2|                1|     4|\n|1879849745|  692759|             1|                0|     0|\n| 128119192| 1197142|             1|                2|     0|\n| 890680799|  478279|             1|                1|     3|\n|2015376879|  576416|             1|               12|     4|\n|1361316829| 1254058|             1|                0|     0|\n|1859946190|  531629|             2|                4|     3|\n|1381923431| 1166965|             2|                6|     1|\n| 425461883|  386531|             2|                0|     1|\n| 598139433| 1207776|             1|                1|     1|\n|1552048840| 1127708|             3|                0|     0|\n| 286000548| 1118878|             2|                0|     0|\n| 785897359|  456662|             2|                5|     4|\n| 543106372|  656427|             1|                1|     0|\n+----------+--------+--------------+-----------------+------+\nonly showing top 20 rows\n\nLoaded successfully\n"
          ]
        }
      ],
      "execution_count": 69,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}