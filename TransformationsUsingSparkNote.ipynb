{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "Pipeline_Id = 1            \r\n",
        "Pipeline_Name = \"LoadingDataFromSourceToSilver\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "jdbc_url = \"jdbc:sqlserver://sqlserver8341.database.windows.net:1433;database=GoldLayer\"\r\n",
        "connection_properties = {\r\n",
        "    \"user\": \"suneetha\",\r\n",
        "    \"password\": \"Suni@123\",\r\n",
        "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.functions import regexp_replace,col,substring, substring_index, instr,concat_ws,md5,to_timestamp,lit,monotonically_increasing_id, lit,current_timestamp\r\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType,DateType,TimestampType\r\n",
        "import re\r\n",
        "\r\n",
        "source_schema = StructType([\r\n",
        "                        StructField(\"id\", IntegerType(), True),\r\n",
        "                        StructField(\"batter\", StringType(), True),\r\n",
        "                        StructField(\"bowler\", StringType(), True),\r\n",
        "                        StructField(\"non_striker\", StringType(), True),\r\n",
        "                        StructField(\"runs_batter\", IntegerType(), True),\r\n",
        "                        StructField(\"runs_extras\", IntegerType(), True),\r\n",
        "                        StructField(\"runs_total\", IntegerType(), True),\r\n",
        "                        StructField(\"delivery_number\", IntegerType(), True),\r\n",
        "                        StructField(\"over_number\", IntegerType(), True),  \r\n",
        "                        StructField(\"team\", StringType(), True),\r\n",
        "                        StructField(\"inning_number\", IntegerType(), True),\r\n",
        "                        StructField(\"match_name\", StringType(), True),\r\n",
        "                        StructField(\"match_id\", StringType(), True),\r\n",
        "                        StructField(\"match_city\", StringType(), True),\r\n",
        "                        StructField(\"match_venue\", StringType(), True),\r\n",
        "                        StructField(\"match_type\", StringType(), True),\r\n",
        "                        StructField(\"team_type\", StringType(), True),\r\n",
        "                        StructField(\"match_start_date\", DateType(), True),\r\n",
        "                        StructField(\"player_out\", StringType(), True),\r\n",
        "                        StructField(\"_corrupt_record\", StringType(), True)\r\n",
        "\r\n",
        "                    ])\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def process_files():\r\n",
        "        mssparkutils.fs.mount(\r\n",
        "            \"abfss://bronzelayer@gen2storageforazurepoc.dfs.core.windows.net/\",\r\n",
        "            \"/mountname\",\r\n",
        "            {\"linkedService\" : \"synapsepoc123-WorkspaceDefaultStorage\"}\r\n",
        "        )\r\n",
        "\r\n",
        "        files = mssparkutils.fs.ls(f'file:{mssparkutils.fs.getMountPath(\"/mountname\")}')\r\n",
        "\r\n",
        "        pattern = r\"(\\d{4}-\\d{2}-\\d{2}) (\\d{2}:\\d{2}:\\d{2})\"\r\n",
        "\r\n",
        "        destination = spark.sql(\"SELECT COALESCE(MAX(date_time), CAST('1999-01-01' AS TIMESTAMP)) AS max_date_time FROM Cricket_Analysis_Data\").collect()\r\n",
        "        max_date_time = destination[0][\"max_date_time\"] if destination and destination[0][\"max_date_time\"] else None\r\n",
        "\r\n",
        "        # looping each file in the directory \r\n",
        "        for file_info in files:\r\n",
        "            file_path = file_info.path\r\n",
        "            file_name = file_info.name      \r\n",
        "            match = re.search(pattern, file_name)\r\n",
        "            \r\n",
        "            if match:\r\n",
        "                date_time = f\"{match.group(1)} {match.group(2)}\"\r\n",
        "            \r\n",
        "                df = spark.createDataFrame([(file_name, date_time)], [\"file_name\", \"date_time\"])\r\n",
        "                df = df.withColumn(\"date_time\", to_timestamp(\"date_time\", \"yyyy-MM-dd HH:mm:ss\"))\r\n",
        "\r\n",
        "                if max_date_time:\r\n",
        "                    if df.collect()[0][\"date_time\"] > max_date_time:\r\n",
        "                        print(f\"Source date_time ({df.collect()[0]['date_time']}) is newer. Proceed with processing {file_name}.\")\r\n",
        "\r\n",
        "                        path = f\"abfss://bronzelayer@gen2storageforazurepoc.dfs.core.windows.net/{file_name}\"\r\n",
        "                        print(path)\r\n",
        "                    \r\n",
        "                        stage_df = spark.read.option(\"header\",\"true\").schema(source_schema).option(\"mode\", \"PERMISSIVE\").option(\"columnNameOfCorruptRecord\", \"_corrupt_record\").csv(path)\r\n",
        "                        #display(stage_df)\r\n",
        "                        \r\n",
        "                        stage_df.cache()\r\n",
        "\r\n",
        "                        cn_df = stage_df.where(col(\"_corrupt_record\").isNotNull()).count()\r\n",
        "\r\n",
        "                        if cn_df>0:\r\n",
        "\r\n",
        "                            #Handling Corrupted Records...\r\n",
        "                            print(\"Corrupted Records Found. Started loading into corruptedrecords container and also in process_corrupt_record table... \")\r\n",
        "\r\n",
        "                            currupted_records= stage_df.where(col(\"_corrupt_record\").isNotNull())\r\n",
        "                            currupted_records.write.option(\"header\",True).mode(\"append\").csv(\"abfss://corruptedrecords@gen2storageforazurepoc.dfs.core.windows.net/corrupted records/\")\r\n",
        "\r\n",
        "                            currupted_records_df = stage_df \\\r\n",
        "                                .withColumn(\"Pipeline_Id\", lit(Pipeline_Id)) \\\r\n",
        "                                .withColumn(\"Pipeline_Name\", lit(Pipeline_Name)) \\\r\n",
        "                                .withColumn(\"file_name\", lit(file_name)) \\\r\n",
        "                                .withColumn(\"path\", lit(path)) \\\r\n",
        "                                .withColumn(\"date_time\", to_timestamp(current_timestamp(), \"yyyy-MM-dd HH:mm:ss\")) \\\r\n",
        "                                .select(\"Pipeline_Id\", \"Pipeline_Name\", \"file_name\", \"path\", \"_corrupt_record\", \"date_time\").where(col(\"_corrupt_record\").isNotNull())\r\n",
        "\r\n",
        "                            currupted_records_df.write.jdbc(url=jdbc_url, table=\"logs.process_corrupt_record\", mode=\"append\", properties=connection_properties)\r\n",
        "                            print(\"Corrupted records loaded into a storage and processed_corrupt_record table successfully\")\r\n",
        "                        else:\r\n",
        "                            print(\"No corrupted Records found\")\r\n",
        "\r\n",
        "                        # Processing Non corrupted records\r\n",
        "                        print(\"Processing Non corrupted records.......\")\r\n",
        "                        source_df = stage_df.where(col(\"_corrupt_record\").isNull()).drop(\"_corrupt_record\")\r\n",
        "                        display(source_df)\r\n",
        "\r\n",
        "                        #extracting the date time from the file\r\n",
        "                        file_path = f\"{path}\"\r\n",
        "                        pattern = r\"(\\d{4}-\\d{2}-\\d{2}) (\\d{2}:\\d{2}:\\d{2})\"\r\n",
        "                        match = re.search(pattern, file_path)\r\n",
        "\r\n",
        "                        if match:\r\n",
        "                            date_time = f\"{match.group(1)} {match.group(2)}\"\r\n",
        "                            print(date_time)\r\n",
        "                        \r\n",
        "                        else:\r\n",
        "                            print(\"No date and time found in the file name.\")\r\n",
        "\r\n",
        "\r\n",
        "                        # working on transformations\r\n",
        "                        transforming_source_df = source_df.withColumn(\"batter\", regexp_replace(col(\"batter\"), \"[^a-zA-Z0-9- ]\", \"\"))\\\r\n",
        "                                            .withColumn(\"bowler\", regexp_replace(col(\"bowler\"), \"[^a-zA-Z0-9- ]\", \"\"))\\\r\n",
        "                                            .withColumn(\"non_striker\", regexp_replace(col(\"non_striker\"), \"[^a-zA-Z0-9- ]\", \"\"))\\\r\n",
        "                                            .withColumn(\"player_out\", regexp_replace(col(\"player_out\"), \"-\", \"\"))\\\r\n",
        "                                            .fillna({\"runs_batter\": 0, \"runs_extras\": 0,\"runs_total\": 0})\\\r\n",
        "                                            .withColumn(\"match_id\", substring_index(col(\"match_id\"), \".\", 1))\\\r\n",
        "                                            .withColumn(\"date_time\", to_timestamp(lit(date_time), \"yyyy-MM-dd HH:mm:ss\"))\r\n",
        "\r\n",
        "                        #Generating hash key\r\n",
        "                        df_with_hash = transforming_source_df.withColumn(\r\n",
        "                            \"hash_key\", \r\n",
        "                            md5(\r\n",
        "                                concat_ws(\r\n",
        "                                    \"_\", \r\n",
        "                                    col(\"batter\"),\r\n",
        "                                    col(\"bowler\"),\r\n",
        "                                    col(\"non_striker\"),\r\n",
        "                                    col(\"runs_batter\"),\r\n",
        "                                    col(\"runs_extras\"),\r\n",
        "                                    col(\"runs_total\"),\r\n",
        "                                    col(\"delivery_number\"),\r\n",
        "                                    col(\"over_number\"),\r\n",
        "                                    col(\"inning_number\"),\r\n",
        "                                    col(\"match_name\"),\r\n",
        "                                    col(\"match_id\"),\r\n",
        "                                    col(\"match_city\"),\r\n",
        "                                    col(\"match_venue\"),\r\n",
        "                                    col(\"match_type\"),\r\n",
        "                                    col(\"team_type\"),\r\n",
        "                                    col(\"match_start_date\"),\r\n",
        "                                    col(\"player_out\")\r\n",
        "                                )\r\n",
        "                            )\r\n",
        "                        )\r\n",
        "                        \r\n",
        "                        #display(df_with_hash)\r\n",
        "                        print(\"Hash code Generated\")\r\n",
        "\r\n",
        "                        df_with_hash.createOrReplaceGlobalTempView(\"df_with_hash_table\")\r\n",
        "                        print(\"Hash dataframe converted to temp table\")\r\n",
        "\r\n",
        "                        spark.sql(f\"\"\"MERGE INTO {dest_table_name} AS target\r\n",
        "                        USING global_temp.df_with_hash_table AS source\r\n",
        "                        ON target.hash_key = source.hash_key\r\n",
        "                        WHEN NOT MATCHED THEN\r\n",
        "                            INSERT (id, batter, bowler, non_striker, runs_batter, runs_extras, runs_total, delivery_number, over_number, team, inning_number, match_name, match_id, match_city, match_venue, match_type, team_type, match_start_date, player_out,date_time,hash_key)\r\n",
        "                            VALUES (source.id, source.batter, source.bowler, source.non_striker, source.runs_batter, source.runs_extras, source.runs_total, source.delivery_number, source.over_number, source.team, source.inning_number, source.match_name, source.match_id, source.match_city, source.match_venue, source.match_type, source.team_type, source.match_start_date, source.player_out,source.date_time,source.hash_key);\r\n",
        "                        \"\"\")\r\n",
        "                        print(f\"Inserted data into {dest_table_name}\")\r\n",
        "                         \r\n",
        "                    else:\r\n",
        "                        print(\"No need to process. Already processed Data\")\r\n",
        "                else:\r\n",
        "                    print(\"No need to process\")\r\n",
        "            else:\r\n",
        "                print(f\"No date and time found in the file name: {file_name}\")\r\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "dest_database_name = \"default\"\r\n",
        "dest_schema_name = \"dbo\"\r\n",
        "dest_table_name = \"Cricket_Analysis_Data\"\r\n",
        "\r\n",
        "result = spark.sql(f\"SHOW TABLES IN {dest_database_name}\").filter(f\"tableName = '{dest_table_name}'\").count()\r\n",
        "\r\n",
        "if result > 0:\r\n",
        "    process_files()\r\n",
        "else:\r\n",
        "    schema = StructType([\r\n",
        "                            StructField(\"id\", IntegerType(), True),\r\n",
        "                            StructField(\"batter\", StringType(), True),\r\n",
        "                            StructField(\"bowler\", StringType(), True),\r\n",
        "                            StructField(\"non_striker\", StringType(), True),\r\n",
        "                            StructField(\"runs_batter\", IntegerType(), True),\r\n",
        "                            StructField(\"runs_extras\", IntegerType(), True),\r\n",
        "                            StructField(\"runs_total\", IntegerType(), True),\r\n",
        "                            StructField(\"delivery_number\", IntegerType(), True),\r\n",
        "                            StructField(\"over_number\", IntegerType(), True),  \r\n",
        "                            StructField(\"team\", StringType(), True),\r\n",
        "                            StructField(\"inning_number\", IntegerType(), True),\r\n",
        "                            StructField(\"match_name\", StringType(), True),\r\n",
        "                            StructField(\"match_id\", StringType(), True),\r\n",
        "                            StructField(\"match_city\", StringType(), True),\r\n",
        "                            StructField(\"match_venue\", StringType(), True),\r\n",
        "                            StructField(\"match_type\", StringType(), True),\r\n",
        "                            StructField(\"team_type\", StringType(), True),\r\n",
        "                            StructField(\"match_start_date\", DateType(), True),\r\n",
        "                            StructField(\"player_out\", StringType(), True),\r\n",
        "                            StructField(\"date_time\", TimestampType(), True),\r\n",
        "                            StructField(\"hash_key\", StringType(), True),\r\n",
        "                        ])\r\n",
        "\r\n",
        "    lake_df = spark.createDataFrame([], schema)\r\n",
        "    lake_df.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{dest_table_name}\")\r\n",
        "    process_files()\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "jdbc_url = \"jdbc:sqlserver://sqlserver8341.database.windows.net:1433;database=GoldLayer\"\r\n",
        "connection_properties = {\r\n",
        "    \"user\": \"suneetha\",\r\n",
        "    \"password\": \"Suni@123\",\r\n",
        "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Gold_Team_Read = spark.read.jdbc(url=jdbc_url, table=\"gold.gl_Team\",  properties=connection_properties)\r\n",
        "Gold_Team_Read.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<b>Team info</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "\r\n",
        "# Load the data\r\n",
        "df = spark.sql(\"\"\"\r\n",
        "    SELECT Team_Name, Team_Type \r\n",
        "        FROM (\r\n",
        "        SELECT DISTINCT team AS Team_Name, team_type AS Team_Type, COUNT(*) AS con\r\n",
        "        FROM cricket_analysis_data\r\n",
        "        GROUP BY team, team_type\r\n",
        "        ) AS teams_dataset\r\n",
        "\"\"\")\r\n",
        "\r\n",
        "Team_df_with_id = df.withColumn(\"Team_ID\", F.abs(F.hash(F.col(\"Team_Name\"), F.col(\"Team_Type\"))))\r\n",
        "\r\n",
        "Team_df = Team_df_with_id.select(\"Team_ID\", \"Team_Name\", \"Team_Type\").distinct()\r\n",
        "\r\n",
        "Gold_Team_Read = spark.read.jdbc(url=jdbc_url, table=\"[gold].[Team_Dim]\",  properties=connection_properties)\r\n",
        "Gold_Team_Read\r\n",
        "\r\n",
        "Gold_Team_Join_df = Team_df.join(Gold_Team_Read, Team_df[\"Team_ID\"] == Gold_Team_Read[\"Team_ID\"],\"left_anti\")\r\n",
        "Gold_Team_Join_df.show()\r\n",
        "\r\n",
        "Gold_Team_Join_df.write.jdbc(url=jdbc_url, table=\"[gold].[Team_Dim]\", mode=\"append\", properties=connection_properties)\r\n",
        "print(\"Loaded successfully\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<b>player info</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "df = spark.sql(\"\"\"\r\n",
        "     SELECT DISTINCT player_name\r\n",
        "        FROM (\r\n",
        "            SELECT DISTINCT batter AS player_name FROM cricket_analysis_data\r\n",
        "            UNION\r\n",
        "            SELECT DISTINCT bowler AS player_name FROM cricket_analysis_data\r\n",
        "        ) AS player_data\r\n",
        "\"\"\")\r\n",
        "\r\n",
        "player_df_with_id = df.withColumn(\"Player_ID\", F.abs(F.hash(F.col(\"player_name\"))))\r\n",
        "\r\n",
        "players_df = player_df_with_id.select(\"Player_ID\", \"Player_Name\").distinct()\r\n",
        "\r\n",
        "#players_df.show()\r\n",
        "\r\n",
        "Player_Gold_Read = spark.read.jdbc(url=jdbc_url, table=\"[gold].[Player_Dim]\",  properties=connection_properties)\r\n",
        "#Player_Gold_Read.show()\r\n",
        "\r\n",
        "Player_Gold_Join_df = players_df.join(Player_Gold_Read, players_df[\"Player_ID\"] == Player_Gold_Read[\"Player_ID\"],\"left_anti\")\r\n",
        "Player_Gold_Join_df.show()\r\n",
        "\r\n",
        "Player_Gold_Join_df.write.jdbc(url=jdbc_url, table=\"[gold].[Player_Dim]\", mode=\"append\", properties=connection_properties)\r\n",
        "print(\"Loaded successfully\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<b> relational table for players and team</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "df = spark.sql(\"\"\"\r\n",
        "SELECT data.match_venue AS Match_Venue\r\n",
        "\t,data.match_city AS Match_City\r\n",
        "FROM (\r\n",
        "\tSELECT DISTINCT match_venue\r\n",
        "\t\t,match_city\r\n",
        "\t\t,row_number() OVER (\r\n",
        "\t\t\tPARTITION BY match_venue ORDER BY match_city DESC\r\n",
        "\t\t\t) AS rownum\r\n",
        "\tFROM stage_match_details\r\n",
        "    Where Match_Venue IS NOT NULL\r\n",
        "\t) AS data\r\n",
        "WHERE rownum = 1\r\n",
        "\"\"\")\r\n",
        "\r\n",
        "Location_df_with_id = df.withColumn(\"Location_ID\", F.abs(F.hash(F.col(\"match_venue\"))))\r\n",
        "\r\n",
        "Location_df = Location_df_with_id.select(\"Location_ID\", \"Match_Venue\",\"Match_City\").distinct()\r\n",
        "\r\n",
        "Location_df.show()\r\n",
        "\r\n",
        "# Reading data from the gold layer from location table\r\n",
        "\r\n",
        "Location_Gold_Read = spark.read.jdbc(url=jdbc_url, table=\"[gold].[Location_Dim]\",  properties=connection_properties)\r\n",
        "Location_Gold_Read.show()\r\n",
        "\r\n",
        "Location_Gold_Join_df = Location_df.join(Location_Gold_Read, Location_df[\"Location_ID\"] == Location_Gold_Read[\"Location_ID\"],\"left_anti\")\r\n",
        "Location_Gold_Join_df.show()\r\n",
        "\r\n",
        "Location_Gold_Join_df.write.jdbc(url=jdbc_url, table=\"[gold].[Location_Dim]\", mode=\"append\", properties=connection_properties)\r\n",
        "print(\"Loaded successfully\")\r\n",
        "\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "df = spark.sql(\"\"\"\r\n",
        "     select DISTINCT player_name,team  FROM (\r\n",
        "    select distinct batter as player_name,team from cricket_analysis_data\r\n",
        "    UNION\r\n",
        "    select distinct bowler as player_name,team from cricket_analysis_data\r\n",
        "    ) as player_data \r\n",
        "\"\"\")\r\n",
        "\r\n",
        "\r\n",
        "player_join_df = df.join(Team_df, df.team == Team_df.Team_Name,\"inner\") \\\r\n",
        "                 .join(players_df1, df.player_name == players_df1.Player_Name,\"inner\")\r\n",
        "\r\n",
        "\r\n",
        "player_team_df = player_join_df.select(\"Player_ID\", \"Team_ID\").distinct()\r\n",
        "\r\n",
        "player_team_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"stage_Player_Team_Relation\")\r\n",
        "\r\n",
        "\r\n",
        "# player_team_df.write.jdbc(url=jdbc_url, table=\"Stage.stage_Player_Team_Relation\", mode=\"overwrite\", properties=connection_properties)\r\n",
        "\r\n",
        "# print(\"Loaded successfully\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<b> Match Info</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "df = spark.sql(\"\"\"\r\n",
        "SELECT ca.match_id as Match_Id\r\n",
        "\t,ca.match_name as Match_Name\r\n",
        "\t,match_type as Match_Type\r\n",
        "\t,Match_venue as Match_Venue\r\n",
        "\t,inning_number as Inning_Number\r\n",
        "\t,match_start_date as Match_Start_Date\r\n",
        "FROM cricket_analysis_data ca\r\n",
        "JOIN stage_Match_Location b ON ca.match_venue = b.match_venue\r\n",
        "\r\n",
        "\"\"\")\r\n",
        "\r\n",
        "# Match_df = df.select(\"Match_Id\", \"Match_Name\",\"Match_Type\",\"Inning_Number\",\"Match_Venue\",\"Match_Start_Date\").orderBy(\"match_id\")\r\n",
        "\r\n",
        "# Match_df.show()\r\n",
        "\r\n",
        "Join_match_location = df.join(Location_df, Match_df.Match_Venue == Location_df.Match_Venue,\"inner\")\r\n",
        "\r\n",
        "# Match_df = df.select(\"Match_Id\", \"Match_Name\",\"Match_Type\",\"Inning_Number\",\"Match_Venue\",\"Match_Start_Date\").orderBy(\"match_id\")\r\n",
        "\r\n",
        "Join_match_location.show()\r\n",
        "\r\n",
        "#Match_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"Fact_Match_Details\")\r\n",
        "\r\n",
        "\r\n",
        "# Match_df.write.jdbc(url=jdbc_url, table=\"Stage.stage_Match_Details\", mode=\"overwrite\", properties=connection_properties)\r\n",
        "\r\n",
        "# print(\"Loaded successfully\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<b> Creating Fact Table </b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "# Create the initial DataFrame from a SQL query\r\n",
        "events_df = spark.sql(\"\"\"\r\n",
        "SELECT batter          AS Batter,\r\n",
        "       bowler          AS Bowler,\r\n",
        "       non_striker     AS Non_Striker,\r\n",
        "       runs_batter     AS Runs_Batter,\r\n",
        "       runs_extras     AS Runs_Extras, \r\n",
        "       runs_total      AS Runs_Total,\r\n",
        "       delivery_number AS Delivery_Number,\r\n",
        "       over_number     AS Over_Number,\r\n",
        "       inning_number   AS Inning_Number,\r\n",
        "       team            AS Team,\r\n",
        "       team_type       AS Team_Type,\r\n",
        "       match_id        AS Match_Id,\r\n",
        "       player_out      AS Player_Out,\r\n",
        "       date_time       AS Silver_Load_date\r\n",
        "FROM cricket_analysis_data\r\n",
        "\"\"\")\r\n",
        "print(events_df.count())\r\n",
        "\r\n",
        "\r\n",
        "Team_join_df = events_df.join(Team_df,events_df.Team == Team_df.Team_Name, \"left\").distinct()\r\n",
        "print(Team_join_df.count())\r\n",
        "\r\n",
        "result_df = Team_join_df.drop(\"Team\",\"Team_Type\",\"Team_Name\")\r\n",
        "\r\n",
        "batter_join_df = (result_df.join(players_df1, result_df.Batter == players_df1.Player_Name, \"left\"))\r\n",
        "\r\n",
        "\r\n",
        "player_join_df = batter_join_df.select(\r\n",
        "    F.col(\"Player_ID\").alias(\"Batter_Id\"),  \r\n",
        "    result_df[\"Batter\"], \r\n",
        "    result_df[\"Bowler\"], \r\n",
        "    result_df[\"Non_Striker\"], \r\n",
        "    result_df[\"Runs_Batter\"], \r\n",
        "    result_df[\"Runs_Extras\"], \r\n",
        "    result_df[\"Runs_Total\"], \r\n",
        "    result_df[\"Delivery_Number\"], \r\n",
        "    result_df[\"Over_Number\"], \r\n",
        "    result_df[\"Inning_Number\"], \r\n",
        "    result_df[\"Team_ID\"],\r\n",
        "    result_df[\"Match_Id\"], \r\n",
        "    result_df[\"Player_Out\"], \r\n",
        "    result_df[\"Silver_Load_date\"]\r\n",
        ")\r\n",
        "\r\n",
        "#display(player_join_df)\r\n",
        "display(player_join_df.count())\r\n",
        "\r\n",
        "Blower_join_df = (result_df.join(players_df1, result_df.Bowler == players_df1.Player_Name, \"left\"))\r\n",
        "\r\n",
        "player_join_df1 = Blower_join_df.select(\r\n",
        "    result_df[\"Batter\"], \r\n",
        "    F.col(\"Player_ID\").alias(\"Bowler_Id\"),  \r\n",
        "    result_df[\"Bowler\"], \r\n",
        "    result_df[\"Non_Striker\"], \r\n",
        "    result_df[\"Runs_Batter\"], \r\n",
        "    result_df[\"Runs_Extras\"], \r\n",
        "    result_df[\"Runs_Total\"], \r\n",
        "    result_df[\"Delivery_Number\"], \r\n",
        "    result_df[\"Over_Number\"], \r\n",
        "    result_df[\"Inning_Number\"], \r\n",
        "    result_df[\"Team_ID\"],\r\n",
        "    result_df[\"Match_Id\"], \r\n",
        "    result_df[\"Player_Out\"], \r\n",
        "    result_df[\"Silver_Load_date\"]\r\n",
        ")\r\n",
        "\r\n",
        "display(player_join_df1.count())\r\n",
        "\r\n",
        "two_pair = player_join_df1.join(player_join_df, player_join_df1.Batter == player_join_df.Batter, \"inner\").select(player_join_df[\"Batter_Id\"],player_join_df1[\"*\"])\r\n",
        "\r\n",
        "# Select desired columns\r\n",
        "#display(two_pair)\r\n",
        "\r\n",
        "#section 3\r\n",
        "\r\n",
        "non_stricket_join_df = (two_pair.join(players_df1, two_pair.Non_Striker == players_df1.Player_Name, \"left\"))\r\n",
        "\r\n",
        "three_df = non_stricket_join_df.select(non_stricket_join_df[\"Player_ID\"].alias(\"non_stricker_id\"),two_pair[\"*\"])\r\n",
        "\r\n",
        "final_df1 = three_df.select(\r\n",
        "    three_df[\"Batter_Id\"], \r\n",
        "    three_df[\"Bowler_Id\"], \r\n",
        "    three_df[\"non_stricker_id\"], \r\n",
        "    three_df[\"Runs_Batter\"], \r\n",
        "    three_df[\"Runs_Extras\"], \r\n",
        "    three_df[\"Runs_Total\"], \r\n",
        "    three_df[\"Delivery_Number\"], \r\n",
        "    three_df[\"Over_Number\"], \r\n",
        "    three_df[\"Inning_Number\"], \r\n",
        "    three_df[\"Team_ID\"],\r\n",
        "    three_df[\"Match_Id\"], \r\n",
        "    three_df[\"Player_Out\"], \r\n",
        "    three_df[\"Silver_Load_date\"]\r\n",
        ").distinct()\r\n",
        "print(\"done\")\r\n",
        "display(final_df1.count())\r\n",
        "\r\n",
        "#section 4\r\n",
        "Player_Out_join_df = (three_df.join(players_df1, three_df.Player_Out == players_df1.Player_Name, \"left\"))\r\n",
        "\r\n",
        "Four_df = Player_Out_join_df.select(Player_Out_join_df[\"Player_ID\"].alias(\"Player_Out_Id\"),three_df[\"*\"])\r\n",
        "\r\n",
        "final_df2 = Four_df.select(\r\n",
        "    Four_df[\"Batter_Id\"], \r\n",
        "    Four_df[\"Bowler_Id\"], \r\n",
        "    Four_df[\"non_stricker_id\"], \r\n",
        "    Four_df[\"Runs_Batter\"], \r\n",
        "    Four_df[\"Runs_Extras\"], \r\n",
        "    Four_df[\"Runs_Total\"], \r\n",
        "    Four_df[\"Delivery_Number\"], \r\n",
        "    Four_df[\"Over_Number\"], \r\n",
        "    Four_df[\"Inning_Number\"], \r\n",
        "    Four_df[\"Team_ID\"],\r\n",
        "    Four_df[\"Match_Id\"], \r\n",
        "    Four_df[\"Player_Out_Id\"], \r\n",
        "    Four_df[\"Silver_Load_date\"]\r\n",
        ").distinct()\r\n",
        "display(final_df2.count())\r\n",
        "\r\n",
        "print(\"done section 4\")\r\n",
        "\r\n",
        "# final_df2.write.jdbc(url=jdbc_url, table=\"Stage.stage_Fact_Cricket_Details\", mode=\"overwrite\", properties=connection_properties)\r\n",
        "\r\n",
        "# print(\"Loaded successfully\")\r\n",
        "\r\n",
        "final_df2.write.format(\"delta\").mode(\"append\").saveAsTable(\"stage_Main_Cricket_Details\")\r\n",
        "\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "select distinct a.Match_Id,a.Team_ID from \r\n",
        "cricket_analysis_data a\r\n",
        "join stage.stage_Match_Details b\r\n",
        "on a.Match_Id = b.Match_Id\r\n",
        "join Stage.stage_Team c\r\n",
        "on a.Team_ID = c.Team_ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}